DFS CRAWLER -------------------------------------------------------------------

INPUT
-->     Each crawler script utilizes command line arguments in order to fetch data.
-->     Calling a script without arguments will provide usage instructions.
            ie. >> dfs.py
                >> Usage: dfs.py [starting URL] [depth] [query term (optional)]

OUTPUT
    Crawler data is output via STDOUT as collected.
    Data output is formatted into JSON.

-->     A JSON string is created for each page, formatted as follows:
        {"id": int, "depth": int, "title": "str", "url": "str", "dead": int, "found": int, "links": int, "children": [list of ints]}
        
-->     ON JSON KEY/VALUE PAIRS:
        id:     Unique id of the page, assigned in order of discovery. The starting URL is assigned an id value of 0.
        depth:  The "tree depth" of the page. The depth of the starting URL is always 0, and all of its children are 1.
        title:  The string text found within the html title tags of the page.
                If the URL is not of html/text type, a value of "No Title" is given.
        url:    The URL of the page in string format.
        dead:   A boolean value signifying the status of the page.
                Bad status codes will set this value to 1, and its data will not be parsed.
        found:  A boolean value signifying whether the user's query term was found on the page.
                A found query sets this value to 1.
        links:  The number of scrapable links on the page. Also the length of the page's 'children' array.
                Href values beginning with "#" or containing empty strings are not included.
        children:   A list of 'child' links found on the page. These are in the form of id values.
        
-->     When crawler data is output, it follows the following JSON string format:
    
            { "Results": [
                { "node": {PARENT_1_DATA}, "leaves": [{CHILD_1_DATA}, {CHILD_2_DATA} ... {CHILD_N_DATA}] },
                { "node": {PARENT_2_DATA}, "leaves": [{CHILD_1_DATA}, {CHILD_2_DATA} ... {CHILD_N_DATA}] },
                ...
                { "node": {PARENT_M_DATA}, "leaves": [{CHILD_1_DATA}, {CHILD_2_DATA} ... {CHILD_N_DATA}] }
            ]}

        
        The first index of the Results array corresponds with the depth level is was scraped from. So...
            Results[0]['node']          : refers to the first parent(starting page) data.
            Results[1]['leaves']        : refers to the second parent's child array data.
            Results[1]['leaves'][4]     : refers to the data of the second parent's 5th child.
            Results[1]['node']['title'] : refers to the second parent's page title.

        * NOTE: The list of child links that follows their parent holds 0 values for 'link' and 'children' fields.
                Valid 'links' and 'children' fields are only given if a child link is selected as a new parent.
                Valid 'links' and 'children' field data will be given once the child is presented as the new parent.
